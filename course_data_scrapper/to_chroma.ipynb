{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "696a39a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_30396\\3932452636.py:78: LangChainDeprecationWarning: The class `Chroma` was deprecated in LangChain 0.2.9 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-chroma package and should be used instead. To use it run `pip install -U :class:`~langchain-chroma` and import as `from :class:`~langchain_chroma import Chroma``.\n",
      "  vectordb = Chroma(persist_directory=CHROMA_DIR, embedding_function=embedding_model)\n",
      "Embedding batches: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8/8 [01:55<00:00, 14.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… å®Œæˆï¼šç¸½å…±è™•ç† 1300 ç­†èª²ç¨‹ï¼Œçµæœå·²å„²å­˜æ–¼ ../persist/chroma_data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_30396\\3932452636.py:88: LangChainDeprecationWarning: Since Chroma 0.4.x the manual persistence method is no longer supported as docs are automatically persisted.\n",
      "  vectordb.persist()\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "from typing import List\n",
    "from langchain.docstore.document import Document\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from tenacity import retry, stop_after_attempt, wait_random_exponential\n",
    "import tiktoken\n",
    "\n",
    "RateLimitError = Exception\n",
    "\n",
    "# ====== CONFIG ======\n",
    "SOURCE_JSON = \"parsed_course_data.json\"\n",
    "CHROMA_DIR = \"persist/chroma_data\"\n",
    "OPENAI_API_KEY = os.environ[\"OPENAI_API_KEY\"]\n",
    "\n",
    "EMBEDDING_MODEL_NAME = \"text-embedding-ada-002\"\n",
    "MAX_TOKENS_PER_BATCH = 250000\n",
    "BATCH_DELAY_SECONDS = 1.5\n",
    "# =====================\n",
    "\n",
    "# âœ… åˆå§‹åŒ–\n",
    "embedding_model = OpenAIEmbeddings(api_key=OPENAI_API_KEY)\n",
    "tokenizer = tiktoken.encoding_for_model(EMBEDDING_MODEL_NAME)\n",
    "\n",
    "# âœ… è¼‰å…¥èª²ç¨‹è³‡æ–™\n",
    "with open(SOURCE_JSON, \"r\", encoding=\"utf-8\") as f:\n",
    "    raw_courses = json.load(f)\n",
    "\n",
    "\n",
    "# âœ… æ™‚é–“æ¬„ä½è½‰ metadataï¼ˆå«å­—ä¸²èˆ‡å¸ƒæ—æ¨™è¨˜ï¼‰\n",
    "def build_time_metadata(time_slots):\n",
    "    if isinstance(time_slots, list):\n",
    "        time_str = \",\".join(time_slots)\n",
    "        time_flags = {f\"ts_{ts}\": True for ts in time_slots}\n",
    "    else:\n",
    "        time_str = \"\"\n",
    "        time_flags = {}\n",
    "    return {\"time_slots\": time_str, **time_flags}\n",
    "\n",
    "\n",
    "# âœ… å»ºç«‹ Document åˆ—è¡¨\n",
    "documents = []\n",
    "for c in raw_courses:\n",
    "    content = f\"èª²ç¨‹åç¨±ï¼š{c['title']}\\nèª²ç¨‹ä»‹ç´¹ï¼š{c['description']}\\næˆèª²è€å¸«ï¼š{c.get('instructor','')}\\nèª²ç¨‹ç¶²å€ï¼š{c.get('course_url','')}\"\n",
    "    metadata = {\"course_id\": c[\"course_id\"], \"title\": c[\"title\"], \"instructor\": c.get(\"instructor\", \"\"), \"course_url\": c.get(\"course_url\", \"\"), **build_time_metadata(c.get(\"time_slots\", []))}\n",
    "    documents.append(Document(page_content=content, metadata=metadata))\n",
    "\n",
    "\n",
    "# âœ… è¨ˆç®— token æ•¸\n",
    "def count_tokens(text: str) -> int:\n",
    "    return len(tokenizer.encode(text))\n",
    "\n",
    "\n",
    "# âœ… æŒ‰ token æ•¸åˆ†æ‰¹\n",
    "def batch_by_token_limit(docs: List[Document], max_tokens: int):\n",
    "    batch, total = [], 0\n",
    "    for doc in docs:\n",
    "        tokens = count_tokens(doc.page_content)\n",
    "        if total + tokens > max_tokens and batch:\n",
    "            yield batch\n",
    "            batch, total = [], 0\n",
    "        batch.append(doc)\n",
    "        total += tokens\n",
    "    if batch:\n",
    "        yield batch\n",
    "\n",
    "\n",
    "# âœ… å®‰å…¨å°è£çš„ add_textsï¼Œå…§å»º retry\n",
    "@retry(wait=wait_random_exponential(min=2, max=10), stop=stop_after_attempt(5))\n",
    "def safe_add_texts(vectordb, texts, metadatas):\n",
    "    vectordb.add_texts(texts=texts, metadatas=metadatas)\n",
    "\n",
    "\n",
    "# âœ… åˆå§‹åŒ– Chroma å‘é‡åº«\n",
    "vectordb = Chroma(persist_directory=CHROMA_DIR, embedding_function=embedding_model)\n",
    "\n",
    "# âœ… åˆ†æ‰¹åµŒå…¥èˆ‡å¯«å…¥\n",
    "batches = list(batch_by_token_limit(documents, MAX_TOKENS_PER_BATCH))\n",
    "for i, batch in enumerate(tqdm(batches, desc=\"Embedding batches\")):\n",
    "    texts = [d.page_content for d in batch]\n",
    "    metadatas = [d.metadata for d in batch]\n",
    "    safe_add_texts(vectordb, texts, metadatas)\n",
    "    time.sleep(BATCH_DELAY_SECONDS)\n",
    "\n",
    "vectordb.persist()\n",
    "print(f\"âœ… å®Œæˆï¼šç¸½å…±è™•ç† {len(documents)} ç­†èª²ç¨‹ï¼Œçµæœå·²å„²å­˜æ–¼ {CHROMA_DIR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd074a6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… å·²å£“ç¸®ç‚º chroma_data.zip\n"
     ]
    }
   ],
   "source": [
    "import zipfile\n",
    "import os\n",
    "\n",
    "\n",
    "def zip_chroma_data(folder_path, output_zip=\"chroma_data.zip\"):\n",
    "    with zipfile.ZipFile(output_zip, \"w\", zipfile.ZIP_DEFLATED) as zipf:\n",
    "        for root, _, files in os.walk(folder_path):\n",
    "            for file in files:\n",
    "                filepath = os.path.join(root, file)\n",
    "                arcname = os.path.relpath(filepath, start=folder_path)\n",
    "                zipf.write(filepath, arcname)\n",
    "\n",
    "\n",
    "zip_chroma_data(\"persist/chroma_data\")  # â¬… é€™è£¡æ”¹æˆä½ çš„ Chroma è³‡æ–™å¤¾è·¯å¾‘\n",
    "print(\"âœ… å·²å£“ç¸®ç‚º chroma_data.zip\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6e3dc8b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… æˆåŠŸé€£ç·šï¼Œåˆ—å‡º container ä¸­çš„ blob:\n"
     ]
    }
   ],
   "source": [
    "from azure.storage.blob import BlobServiceClient\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "account_name = os.getenv(\"AZURE_STORAGE_ACCOUNT_NAME\")\n",
    "account_key = os.getenv(\"AZURE_STORAGE_ACCOUNT_KEY\")\n",
    "container_name = os.getenv(\"AZURE_BLOB_CONTAINER\")\n",
    "\n",
    "connection_str = f\"DefaultEndpointsProtocol=https;AccountName={account_name};AccountKey={account_key};EndpointSuffix=core.windows.net\"\n",
    "blob_service_client = BlobServiceClient.from_connection_string(connection_str)\n",
    "\n",
    "try:\n",
    "    container_client = blob_service_client.get_container_client(container_name)\n",
    "    print(\"âœ… æˆåŠŸé€£ç·šï¼Œåˆ—å‡º container ä¸­çš„ blob:\")\n",
    "    for blob in container_client.list_blobs():\n",
    "        print(\" -\", blob.name)\n",
    "except Exception as e:\n",
    "    print(\"âŒ é€£ç·šå¤±æ•—ï¼Œè«‹æª¢æŸ¥ account name/key/container name\")\n",
    "    print(\"éŒ¯èª¤è¨Šæ¯ï¼š\", e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e6034fcd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… æ¸¬è©¦æª”æ¡ˆå·²ä¸Šå‚³\n"
     ]
    }
   ],
   "source": [
    "from azure.storage.blob import BlobClient\n",
    "from azure.storage.blob import ContentSettings\n",
    "\n",
    "test_blob_name = \"test-upload.txt\"\n",
    "test_blob_client = blob_service_client.get_blob_client(container=container_name, blob=test_blob_name)\n",
    "\n",
    "with open(\"test-upload.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(\"é€™æ˜¯ä¸€å€‹æ¸¬è©¦æª”æ¡ˆ\")\n",
    "\n",
    "with open(\"test-upload.txt\", \"rb\") as data:\n",
    "    test_blob_client.upload_blob(data, overwrite=True, content_settings=ContentSettings(content_type=\"text/plain\"))\n",
    "    print(\"âœ… æ¸¬è©¦æª”æ¡ˆå·²ä¸Šå‚³\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "35f2e161",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "ğŸ“¤ Uploading: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 23.8M/23.8M [01:03<00:00, 378kB/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "âœ… ä¸Šå‚³å®Œæˆï¼šcourse_vector.zip è‡³ container course-data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from azure.storage.blob import BlobClient, ContentSettings\n",
    "from tqdm import tqdm\n",
    "\n",
    "# --------------- ğŸ”§ è¨­å®šå¸¸æ•¸ ----------------\n",
    "LOCAL_FILE = \"chroma_data.zip\"\n",
    "BLOB_NAME = \"course_vector.zip\"\n",
    "CONTENT_TYPE = \"application/zip\"\n",
    "MAX_SINGLE_PUT_SIZE = 16 * 1024 * 1024  # 16MB\n",
    "MAX_BLOCK_SIZE = 4 * 1024 * 1024  # 4MB\n",
    "TIMEOUT = 600\n",
    "MAX_CONCURRENCY = 4\n",
    "\n",
    "# --------------- ğŸ” è¼‰å…¥æ†‘è­‰ ----------------\n",
    "load_dotenv()\n",
    "ACCOUNT_NAME = os.getenv(\"AZURE_STORAGE_ACCOUNT_NAME\")\n",
    "ACCOUNT_KEY = os.getenv(\"AZURE_STORAGE_ACCOUNT_KEY\")\n",
    "CONTAINER_NAME = os.getenv(\"AZURE_BLOB_CONTAINER\")\n",
    "\n",
    "if not all([ACCOUNT_NAME, ACCOUNT_KEY, CONTAINER_NAME]):\n",
    "    raise ValueError(\"âŒ è«‹ç¢ºèª .env ä¸­å¸³è™Ÿè³‡è¨Šæ˜¯å¦é½Šå…¨\")\n",
    "\n",
    "# --------------- ğŸ”— å»ºç«‹é€£ç·šå­—ä¸² ----------------\n",
    "AZURE_CONN_STR = f\"DefaultEndpointsProtocol=https;AccountName={ACCOUNT_NAME};AccountKey={ACCOUNT_KEY};EndpointSuffix=core.windows.net\"\n",
    "\n",
    "\n",
    "# --------------- ğŸ“¦ æº–å‚™ä¸Šå‚³æª”æ¡ˆ ----------------\n",
    "class TqdmUploadWrapper:\n",
    "    def __init__(self, file, total):\n",
    "        self.file = file\n",
    "        self.progress_bar = tqdm(total=total, unit=\"B\", unit_scale=True, desc=\"ğŸ“¤ Uploading\")\n",
    "\n",
    "    def read(self, size):\n",
    "        data = self.file.read(size)\n",
    "        self.progress_bar.update(len(data))\n",
    "        return data\n",
    "\n",
    "    def __getattr__(self, attr):\n",
    "        return getattr(self.file, attr)\n",
    "\n",
    "\n",
    "# --------------- â˜ï¸ å»ºç«‹ BlobClient ä¸¦ä¸Šå‚³ ----------------\n",
    "file_size = os.path.getsize(LOCAL_FILE)\n",
    "\n",
    "blob_client = BlobClient.from_connection_string(conn_str=AZURE_CONN_STR, container_name=CONTAINER_NAME, blob_name=BLOB_NAME, max_single_put_size=MAX_SINGLE_PUT_SIZE, max_block_size=MAX_BLOCK_SIZE)\n",
    "\n",
    "with open(LOCAL_FILE, \"rb\") as f:\n",
    "    wrapped = TqdmUploadWrapper(f, total=file_size)\n",
    "    blob_client.upload_blob(data=wrapped, blob_type=\"BlockBlob\", overwrite=True, content_settings=ContentSettings(content_type=CONTENT_TYPE), max_concurrency=MAX_CONCURRENCY, timeout=TIMEOUT)\n",
    "    wrapped.progress_bar.close()\n",
    "\n",
    "print(f\"\\nâœ… ä¸Šå‚³å®Œæˆï¼š{BLOB_NAME} è‡³ container {CONTAINER_NAME}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv311",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
