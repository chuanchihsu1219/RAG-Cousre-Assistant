{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0262400f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openpyxl import load_workbook\n",
    "from tqdm import tqdm\n",
    "import csv\n",
    "\n",
    "# 讀取 Excel 檔案\n",
    "file_path = \"113-1 NTU Course.xlsx\"\n",
    "wb = load_workbook(file_path)\n",
    "ws = wb.active  # 或 wb[\"你的工作表名稱\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2fae2159",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "處理中: 100%|██████████| 1301/1301 [00:00<00:00, 57759.70it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ 已儲存為：113-1_NTU_Course_with_links.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# 準備儲存所有資料（包含 E 欄加入的超連結）\n",
    "data = []\n",
    "\n",
    "# 遍歷列數\n",
    "for row in tqdm(range(1, ws.max_row + 1), desc=\"處理中\"):\n",
    "    row_data = []\n",
    "    for col in range(1, 6):  # 讀取 A~E 欄（1~5）\n",
    "        row_data.append(ws.cell(row=row, column=col).value)\n",
    "\n",
    "    # 將超連結（若有）寫入第 5 欄（E欄）\n",
    "    cell = ws.cell(row=row, column=2)  # B欄是 column 2\n",
    "    if cell.hyperlink:\n",
    "        link = cell.hyperlink.target\n",
    "        row_data[4] = link  # 將第 5 欄覆蓋為超連結\n",
    "    data.append(row_data)\n",
    "\n",
    "# 將資料儲存為 CSV 檔\n",
    "csv_file = \"113-1_NTU_Course_with_links.csv\"\n",
    "with open(csv_file, \"w\", newline=\"\", encoding=\"utf-8-sig\") as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerows(data)\n",
    "\n",
    "print(f\"✅ 已儲存為：{csv_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe93fac5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import threading\n",
    "import requests\n",
    "import queue\n",
    "import csv\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import time\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "# ===== 設定區 =====\n",
    "URL_CSV = '113-1_NTU_Course_with_links.csv'\n",
    "OUTPUT_CSV = 'course_data.csv'\n",
    "ERROR_LOG = 'error_urls.csv'\n",
    "MAX_FETCH_THREADS = 5\n",
    "MAX_PARSE_THREADS = 5\n",
    "HEADERS = {'User-Agent': 'Mozilla/5.0'}\n",
    "\n",
    "# ===== 初始化資料與 queue =====\n",
    "df = pd.read_csv(URL_CSV)\n",
    "fetch_queue = queue.Queue()\n",
    "parse_queue = queue.Queue()\n",
    "writer_queue = queue.Queue()\n",
    "error_list = []\n",
    "\n",
    "for url in df[\"課程連結\"]:\n",
    "    fetch_queue.put(url)\n",
    "\n",
    "# ===== tqdm 記錄數量 =====\n",
    "progress_lock = threading.Lock()\n",
    "fetch_pbar = tqdm(total=fetch_queue.qsize(), desc=\"Fetched\", position=0)\n",
    "parse_pbar = tqdm(total=fetch_queue.qsize(), desc=\"Parsed\", position=1)\n",
    "write_pbar = tqdm(total=fetch_queue.qsize(), desc=\"Written\", position=2)\n",
    "\n",
    "# ===== 抓取階段 =====\n",
    "def fetcher():\n",
    "    while True:\n",
    "        try:\n",
    "            url = fetch_queue.get(timeout=3)\n",
    "        except queue.Empty:\n",
    "            break\n",
    "        try:\n",
    "            response = requests.get(url, headers=HEADERS, timeout=10)\n",
    "            parse_queue.put((url, response.text))\n",
    "        except Exception as e:\n",
    "            print(f\"[抓取失敗] {url}: {e}\")\n",
    "            error_list.append((url, 'fetch', str(e)))\n",
    "        finally:\n",
    "            with progress_lock:\n",
    "                fetch_pbar.update(1)\n",
    "            fetch_queue.task_done()\n",
    "\n",
    "# ===== 解析階段 =====\n",
    "def parser():\n",
    "    while True:\n",
    "        try:\n",
    "            url, html = parse_queue.get(timeout=30)\n",
    "        except queue.Empty:\n",
    "            print(\"Parse queue is empty.\")\n",
    "            break\n",
    "        try:\n",
    "            soup = BeautifulSoup(html, 'html.parser')\n",
    "            # article = soup.find(id='article_page')\n",
    "            text = soup.get_text(separator=\"\\n\", strip=True) if soup else \"\"\n",
    "            writer_queue.put((url, text))\n",
    "        except Exception as e:\n",
    "            print(f\"[解析失敗] {url}: {e}\")\n",
    "            error_list.append((url, 'parse', str(e)))\n",
    "        finally:\n",
    "            with progress_lock:\n",
    "                parse_pbar.update(1)\n",
    "            parse_queue.task_done()\n",
    "\n",
    "# ===== 寫入階段 =====\n",
    "def writer():\n",
    "    with open(OUTPUT_CSV, 'w', newline='', encoding='utf-8') as f:\n",
    "        csv_writer = csv.writer(f)\n",
    "        csv_writer.writerow([\"課程連結\", \"text\"])\n",
    "        while True:\n",
    "            try:\n",
    "                url, text = writer_queue.get(timeout=30)\n",
    "                csv_writer.writerow([url, text])\n",
    "                with progress_lock:\n",
    "                    write_pbar.update(1)\n",
    "                writer_queue.task_done()\n",
    "            except queue.Empty:\n",
    "                print(\"Writer queue is empty.\")\n",
    "                break\n",
    "\n",
    "# ===== 錯誤寫入 =====\n",
    "def write_errors():\n",
    "    if error_list:\n",
    "        with open(ERROR_LOG, 'w', newline='', encoding='utf-8') as f:\n",
    "            writer = csv.writer(f)\n",
    "            writer.writerow(['url', 'stage', 'error'])\n",
    "            writer.writerows(error_list)\n",
    "\n",
    "# ===== 執行流程控制 =====\n",
    "if __name__ == '__main__':\n",
    "    start_time = time.time()\n",
    "\n",
    "    # 啟動所有 thread（平行）\n",
    "    fetch_threads = [threading.Thread(target=fetcher) for _ in range(MAX_FETCH_THREADS)]\n",
    "    parse_threads = [threading.Thread(target=parser) for _ in range(MAX_PARSE_THREADS)]\n",
    "    writer_thread = threading.Thread(target=writer)\n",
    "\n",
    "    for t in fetch_threads + parse_threads:\n",
    "        t.start()\n",
    "    writer_thread.start()\n",
    "\n",
    "    for t in fetch_threads:\n",
    "        t.join()\n",
    "    fetch_queue.join()\n",
    "\n",
    "    for t in parse_threads:\n",
    "        t.join()\n",
    "    parse_queue.join()\n",
    "\n",
    "    writer_thread.join()\n",
    "    writer_queue.join()\n",
    "\n",
    "    # 寫入錯誤紀錄\n",
    "    write_errors()\n",
    "\n",
    "    fetch_pbar.close()\n",
    "    parse_pbar.close()\n",
    "    write_pbar.close()\n",
    "\n",
    "    end_time = time.time()\n",
    "    print(f\"✅ 全部完成，用時 {end_time - start_time:.2f} 秒\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3031a702",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a00ac00899824ed5931c9b717ca08839",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetched:   0%|          | 0/1300 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "959c539d97a24129a39edc678d4d2b5c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Parsed:   0%|          | 0/1300 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "62f4ddc8868c485196628995f33f9ebb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Written:   0%|          | 0/1300 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parse queue is empty.Parse queue is empty.\n",
      "\n",
      "Parse queue is empty.\n",
      "Parse queue is empty.\n",
      "Writer queue is empty.\n",
      "Parse queue is empty.\n",
      "✅ 全部完成，用時 1090.85 秒\n"
     ]
    }
   ],
   "source": [
    "import threading\n",
    "import requests\n",
    "import queue\n",
    "import csv\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import time\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "# ===== 設定區 =====\n",
    "URL_CSV = \"113-1_NTU_Course_with_links.csv\"\n",
    "OUTPUT_CSV = \"course_data.csv\"\n",
    "ERROR_LOG = \"error_urls.csv\"\n",
    "MAX_FETCH_THREADS = 5\n",
    "MAX_PARSE_THREADS = 5\n",
    "HEADERS = {\"User-Agent\": \"Mozilla/5.0\"}\n",
    "\n",
    "# ===== 初始化資料與 queue =====\n",
    "df = pd.read_csv(URL_CSV)\n",
    "fetch_queue = queue.Queue()\n",
    "parse_queue = queue.Queue()\n",
    "writer_queue = queue.Queue()\n",
    "error_list = []\n",
    "\n",
    "for _, row in df.iterrows():\n",
    "    fetch_queue.put(row.to_dict())\n",
    "\n",
    "# ===== tqdm 記錄數量 =====\n",
    "progress_lock = threading.Lock()\n",
    "fetch_pbar = tqdm(total=fetch_queue.qsize(), desc=\"Fetched\", position=0)\n",
    "parse_pbar = tqdm(total=fetch_queue.qsize(), desc=\"Parsed\", position=1)\n",
    "write_pbar = tqdm(total=fetch_queue.qsize(), desc=\"Written\", position=2)\n",
    "\n",
    "\n",
    "# ===== 抓取階段 =====\n",
    "def fetcher():\n",
    "    while True:\n",
    "        try:\n",
    "            row = fetch_queue.get(timeout=3)\n",
    "            url = row[\"課程連結\"]\n",
    "        except queue.Empty:\n",
    "            break\n",
    "        try:\n",
    "            response = requests.get(url, headers=HEADERS, timeout=10)\n",
    "            parse_queue.put((row, response.text))\n",
    "        except Exception as e:\n",
    "            print(f\"[抓取失敗] {url}: {e}\")\n",
    "            error_list.append((url, \"fetch\", str(e)))\n",
    "        finally:\n",
    "            with progress_lock:\n",
    "                fetch_pbar.update(1)\n",
    "            fetch_queue.task_done()\n",
    "\n",
    "\n",
    "# ===== 解析階段 =====\n",
    "def parser():\n",
    "    while True:\n",
    "        try:\n",
    "            row, html = parse_queue.get(timeout=30)\n",
    "            url = row[\"課程連結\"]\n",
    "        except queue.Empty:\n",
    "            print(\"Parse queue is empty.\")\n",
    "            break\n",
    "        try:\n",
    "            soup = BeautifulSoup(html, \"html.parser\")\n",
    "            text = soup.get_text(separator=\"\\n\", strip=True) if soup else \"\"\n",
    "            writer_queue.put((row, text))\n",
    "        except Exception as e:\n",
    "            print(f\"[解析失敗] {url}: {e}\")\n",
    "            error_list.append((url, \"parse\", str(e)))\n",
    "        finally:\n",
    "            with progress_lock:\n",
    "                parse_pbar.update(1)\n",
    "            parse_queue.task_done()\n",
    "\n",
    "\n",
    "# ===== 寫入階段 =====\n",
    "def writer():\n",
    "    with open(OUTPUT_CSV, \"w\", newline=\"\", encoding=\"utf-8-sig\") as f:\n",
    "        csv_writer = None\n",
    "        while True:\n",
    "            try:\n",
    "                row, text = writer_queue.get(timeout=30)\n",
    "                row[\"text\"] = text\n",
    "                if csv_writer is None:\n",
    "                    # 初始化欄位名稱\n",
    "                    fieldnames = list(row.keys())\n",
    "                    csv_writer = csv.DictWriter(f, fieldnames=fieldnames)\n",
    "                    csv_writer.writeheader()\n",
    "                csv_writer.writerow(row)\n",
    "                with progress_lock:\n",
    "                    write_pbar.update(1)\n",
    "                writer_queue.task_done()\n",
    "            except queue.Empty:\n",
    "                print(\"Writer queue is empty.\")\n",
    "                break\n",
    "\n",
    "\n",
    "# ===== 錯誤寫入 =====\n",
    "def write_errors():\n",
    "    if error_list:\n",
    "        with open(ERROR_LOG, \"w\", newline=\"\", encoding=\"utf-8-sig\") as f:\n",
    "            writer = csv.writer(f)\n",
    "            writer.writerow([\"url\", \"stage\", \"error\"])\n",
    "            writer.writerows(error_list)\n",
    "\n",
    "\n",
    "# ===== 執行流程控制 =====\n",
    "if __name__ == \"__main__\":\n",
    "    start_time = time.time()\n",
    "\n",
    "    fetch_threads = [threading.Thread(target=fetcher) for _ in range(MAX_FETCH_THREADS)]\n",
    "    parse_threads = [threading.Thread(target=parser) for _ in range(MAX_PARSE_THREADS)]\n",
    "    writer_thread = threading.Thread(target=writer)\n",
    "\n",
    "    for t in fetch_threads + parse_threads:\n",
    "        t.start()\n",
    "    writer_thread.start()\n",
    "\n",
    "    for t in fetch_threads:\n",
    "        t.join()\n",
    "    fetch_queue.join()\n",
    "\n",
    "    for t in parse_threads:\n",
    "        t.join()\n",
    "    parse_queue.join()\n",
    "\n",
    "    writer_thread.join()\n",
    "    writer_queue.join()\n",
    "\n",
    "    write_errors()\n",
    "\n",
    "    fetch_pbar.close()\n",
    "    parse_pbar.close()\n",
    "    write_pbar.close()\n",
    "\n",
    "    end_time = time.time()\n",
    "    print(f\"✅ 全部完成，用時 {end_time - start_time:.2f} 秒\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
