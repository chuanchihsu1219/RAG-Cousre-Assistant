{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fae2159",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe93fac5",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import threading\n",
    "import requests\n",
    "import queue\n",
    "import csv\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import time\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "# ===== 設定區 =====\n",
    "URL_CSV = '1w_article_count.csv'\n",
    "OUTPUT_CSV = 'text_1w_morethan30reads.csv'\n",
    "ERROR_LOG = 'error_urls.csv'\n",
    "MAX_FETCH_THREADS = 5\n",
    "MAX_PARSE_THREADS = 5\n",
    "HEADERS = {'User-Agent': 'Mozilla/5.0'}\n",
    "\n",
    "# ===== 初始化資料與 queue =====\n",
    "df = pd.read_csv(URL_CSV)\n",
    "df=df[df[\"count\"] >= 30]\n",
    "fetch_queue = queue.Queue()\n",
    "parse_queue = queue.Queue()\n",
    "writer_queue = queue.Queue()\n",
    "error_list = []\n",
    "\n",
    "for url in df['article_url']:\n",
    "    fetch_queue.put(url)\n",
    "\n",
    "# ===== tqdm 記錄數量 =====\n",
    "progress_lock = threading.Lock()\n",
    "fetch_pbar = tqdm(total=fetch_queue.qsize(), desc=\"Fetched\", position=0)\n",
    "parse_pbar = tqdm(total=fetch_queue.qsize(), desc=\"Parsed\", position=1)\n",
    "write_pbar = tqdm(total=fetch_queue.qsize(), desc=\"Written\", position=2)\n",
    "\n",
    "# ===== 抓取階段 =====\n",
    "def fetcher():\n",
    "    while True:\n",
    "        try:\n",
    "            url = fetch_queue.get(timeout=3)\n",
    "        except queue.Empty:\n",
    "            break\n",
    "        try:\n",
    "            response = requests.get(url, headers=HEADERS, timeout=10)\n",
    "            parse_queue.put((url, response.text))\n",
    "        except Exception as e:\n",
    "            print(f\"[抓取失敗] {url}: {e}\")\n",
    "            error_list.append((url, 'fetch', str(e)))\n",
    "        finally:\n",
    "            with progress_lock:\n",
    "                fetch_pbar.update(1)\n",
    "            fetch_queue.task_done()\n",
    "\n",
    "# ===== 解析階段 =====\n",
    "def parser():\n",
    "    while True:\n",
    "        try:\n",
    "            url, html = parse_queue.get(timeout=30)\n",
    "        except queue.Empty:\n",
    "            print(\"Parse queue is empty.\")\n",
    "            break\n",
    "        try:\n",
    "            soup = BeautifulSoup(html, 'html.parser')\n",
    "            article = soup.find(id='article_page')\n",
    "            text = article.get_text(separator='\\n', strip=True) if article else ''\n",
    "            writer_queue.put((url, text))\n",
    "        except Exception as e:\n",
    "            print(f\"[解析失敗] {url}: {e}\")\n",
    "            error_list.append((url, 'parse', str(e)))\n",
    "        finally:\n",
    "            with progress_lock:\n",
    "                parse_pbar.update(1)\n",
    "            parse_queue.task_done()\n",
    "\n",
    "# ===== 寫入階段 =====\n",
    "def writer():\n",
    "    with open(OUTPUT_CSV, 'w', newline='', encoding='utf-8') as f:\n",
    "        csv_writer = csv.writer(f)\n",
    "        csv_writer.writerow(['article_url', 'text'])\n",
    "        while True:\n",
    "            try:\n",
    "                url, text = writer_queue.get(timeout=30)\n",
    "                csv_writer.writerow([url, text])\n",
    "                with progress_lock:\n",
    "                    write_pbar.update(1)\n",
    "                writer_queue.task_done()\n",
    "            except queue.Empty:\n",
    "                print(\"Writer queue is empty.\")\n",
    "                break\n",
    "\n",
    "# ===== 錯誤寫入 =====\n",
    "def write_errors():\n",
    "    if error_list:\n",
    "        with open(ERROR_LOG, 'w', newline='', encoding='utf-8') as f:\n",
    "            writer = csv.writer(f)\n",
    "            writer.writerow(['url', 'stage', 'error'])\n",
    "            writer.writerows(error_list)\n",
    "\n",
    "# ===== 執行流程控制 =====\n",
    "if __name__ == '__main__':\n",
    "    start_time = time.time()\n",
    "\n",
    "    # 啟動所有 thread（平行）\n",
    "    fetch_threads = [threading.Thread(target=fetcher) for _ in range(MAX_FETCH_THREADS)]\n",
    "    parse_threads = [threading.Thread(target=parser) for _ in range(MAX_PARSE_THREADS)]\n",
    "    writer_thread = threading.Thread(target=writer)\n",
    "\n",
    "    for t in fetch_threads + parse_threads:\n",
    "        t.start()\n",
    "    writer_thread.start()\n",
    "\n",
    "    for t in fetch_threads:\n",
    "        t.join()\n",
    "    fetch_queue.join()\n",
    "\n",
    "    for t in parse_threads:\n",
    "        t.join()\n",
    "    parse_queue.join()\n",
    "\n",
    "    writer_thread.join()\n",
    "    writer_queue.join()\n",
    "\n",
    "    # 寫入錯誤紀錄\n",
    "    write_errors()\n",
    "\n",
    "    fetch_pbar.close()\n",
    "    parse_pbar.close()\n",
    "    write_pbar.close()\n",
    "\n",
    "    end_time = time.time()\n",
    "    print(f\"✅ 全部完成，用時 {end_time - start_time:.2f} 秒\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
