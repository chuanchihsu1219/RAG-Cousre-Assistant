# app/routes/utils/rag_chain.py
import os
import re
from app.utils.blob_loader import download_and_extract_chroma_data
from langchain.vectorstores import Chroma
from langchain.embeddings import OpenAIEmbeddings
from langchain_openai import ChatOpenAI
from langchain.prompts import PromptTemplate
from langchain.chains import RetrievalQA
from langchain_core.output_parsers import StrOutputParser
from pydantic import SecretStr
import markdown

# è¨­å®š
BLOB_CONNECTION_STRING = os.environ["AZURE_BLOB_CONNECTION_STRING"]
BLOB_CONTAINER_NAME = "your-container-name"
BLOB_FILE_NAME = "course_vector.zip"
CHROMA_LOCAL_DIR = "./persist/chroma_data"

# ä¸‹è¼‰ä¸¦è§£å£“
# download_and_extract_chroma_data(container_name=BLOB_CONTAINER_NAME, blob_name=BLOB_FILE_NAME, download_dir=CHROMA_LOCAL_DIR, connection_string=BLOB_CONNECTION_STRING)

# åˆå§‹åŒ–
OPENAI_API_KEY = os.environ["OPENAI_API_KEY"]
OPENAI_MODEL = os.environ.get("OPENAI_MODEL", "gpt-4o")

embedding = OpenAIEmbeddings(api_key=OPENAI_API_KEY)
vectordb = Chroma(persist_directory=CHROMA_LOCAL_DIR, embedding_function=embedding)
retriever = vectordb.as_retriever(search_kwargs={"k": 5})

# Prompt
template = """
ä½ æ˜¯å€‹å°ˆæ¥­çš„èª²ç¨‹é¡§å•ï¼Œæœƒæ ¹æ“šå­¸ç”Ÿçš„éœ€æ±‚å¾èª²ç¨‹æè¿°ä¸­æ¨è–¦æœ€é©åˆçš„èª²ã€‚è«‹ä½¿ç”¨ **æ¨™æº– Markdown èªæ³•èˆ‡ç¸®æ’** å›è¦†ï¼Œå‹¿åœ¨æ¸…å–®é …ç›®å‰å¾Œä½¿ç”¨å¤šé¤˜ç¸®æ’æˆ–ç©ºè¡Œï¼Œå› ç‚ºä¹‹å¾Œæœƒå†è¢«æ¸²æŸ“ã€‚
ä»¥ä¸‹æ˜¯å­¸ç”Ÿçš„éœ€æ±‚ï¼š
{question}

ä»¥ä¸‹æ˜¯å¯èƒ½çš„èª²ç¨‹è³‡è¨Šï¼š
{context}

è«‹æ ¹æ“šä¸Šè¿°è³‡è¨Šæ¨è–¦æœ€é©åˆçš„5é–€èª²ï¼Œèªªæ˜åŸå› ï¼Œä¸¦æä¾›èª²åã€æ™‚é–“èˆ‡æµæ°´è™Ÿã€‚
"""
prompt = PromptTemplate.from_template(template)
llm = ChatOpenAI(model=OPENAI_MODEL, api_key=SecretStr(OPENAI_API_KEY), temperature=0.3)

qa_chain = RetrievalQA.from_chain_type(llm=llm, retriever=retriever, return_source_documents=True, chain_type_kwargs={"prompt": prompt})


def clean_markdown(text: str) -> str:
    # import re

    # # 1. å¤šç©ºæ ¼è®Šå…©æ ¼
    # text = re.sub(r"[ ]{3,}", "  ", text)

    # # 2. å¤šé‡æ›è¡Œè®Šä¸€è¡Œ
    text = re.sub(r"\n\s*\n+", "\n", text)

    # # 3. å»é™¤ list å‰çš„ç©ºç™½
    # text = re.sub(r"\n[ ]+(?=- )", "\n- ", text)

    # # 4. åˆä½µè¢«åˆ†æˆå¤šè¡Œçš„ list item æè¿°ï¼ˆé—œéµï¼ï¼‰
    # # æ„å³ï¼šlist å¾Œé¢è·Ÿè‘—çš„ä¸‹ä¸€è¡Œä¸æ˜¯æ–°çš„ listï¼Œå°±ç›´æ¥åˆä½µ
    # text = re.sub(r"(- [^\n]+)\n(?!- )", r"\1 ", text)

    # # 5. ä¿®æ­£é€£çºŒå¤šå€‹ list item æ²’æ›è¡Œåˆ†é–‹çš„æƒ…æ³ï¼ˆoptionalï¼‰
    # # text = re.sub(r"(?<=\n- .+?)\n(?=- )", "\n\n", text)

    return text.strip()


def recommend_course(question, schedule):
    # Step 1ï¼šåˆæ­¥ç›¸ä¼¼åº¦æª¢ç´¢
    raw_docs = vectordb.similarity_search(question, k=15)

    # Step 2ï¼šéæ¿¾ç¬¦åˆã€Œèª²ç¨‹æ™‚é–“ âŠ† ä½¿ç”¨è€… scheduleã€çš„èª²
    filtered_docs = []
    user_set = set(schedule)

    for doc in raw_docs:
        time_slots_str = doc.metadata.get("time_slots", "")
        course_slots = time_slots_str.split(",") if time_slots_str else []
        if set(course_slots).issubset(user_set):
            filtered_docs.append(doc)

    # Step 3ï¼šå¦‚æœæ²’æœ‰ç¬¦åˆçš„èª²ç¨‹
    if not filtered_docs:
        return "æ‰¾ä¸åˆ°ç¬¦åˆæ‚¨æ™‚æ®µçš„èª²ç¨‹ï¼Œè«‹å˜—è©¦èª¿æ•´æ™‚é–“æˆ–æå•å…§å®¹ã€‚"

    # Step 4ï¼šå‚³å…¥ QA chainï¼Œä½†æ”¹æˆç”¨ filtered_docs ç•¶ context
    context = "\n\n".join(doc.page_content for doc in filtered_docs[:5])
    filled_prompt = prompt.format(question=question, context=context)
    result = llm.invoke(filled_prompt)

    # Step 5ï¼šæ¨™æº–åŒ– markdownï¼Œç§»é™¤æ¸…å–®å‰å¤šé¤˜ç©ºç™½ï¼ˆæœ€å¤šä¸‰æ ¼ï¼‰
    print("ğŸ§ª result.content (initial):", repr(result.content))
    cleaned = clean_markdown(result.content)
    print()
    print("ğŸ§ª result.content (cleaned):", repr(cleaned))
    return cleaned
