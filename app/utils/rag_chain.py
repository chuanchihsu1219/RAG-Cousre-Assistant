# app/utils/rag_chain.py
import os
import re
from app.utils.blob_loader import download_and_extract_chroma_data
from langchain.vectorstores import Chroma
from langchain.embeddings import OpenAIEmbeddings
from langchain_openai import ChatOpenAI
from langchain.prompts import PromptTemplate
from langchain.chains import RetrievalQA
from langchain_core.output_parsers import StrOutputParser
from pydantic import SecretStr
import markdown

# è¨­å®š
BLOB_CONNECTION_STRING = os.environ["AZURE_BLOB_CONNECTION_STRING"]
BLOB_CONTAINER_NAME = os.environ["AZURE_BLOB_CONTAINER"]
BLOB_FILE_NAME = os.environ["BLOB_NAME"]
CHROMA_LOCAL_DIR = "./persist/chroma_data"
OPENAI_API_KEY = os.environ["OPENAI_API_KEY"]
OPENAI_MODEL = os.environ.get("OPENAI_MODEL", "gpt-4o")

# å…¨åŸŸè®Šæ•¸ï¼ˆå»¶é²åˆå§‹åŒ–ï¼‰
vectordb = None
qa_chain = None
prompt = PromptTemplate.from_template(
    """
ä½ æ˜¯å€‹å°ˆæ¥­çš„èª²ç¨‹é¡§å•ï¼Œæœƒæ ¹æ“šå­¸ç”Ÿçš„éœ€æ±‚å¾èª²ç¨‹æè¿°ä¸­æ¨è–¦æœ€é©åˆçš„èª²ã€‚è«‹ä½¿ç”¨ **æ¨™æº– Markdown èªæ³•èˆ‡ç¸®æ’** å›è¦†ï¼Œå‹¿åœ¨æ¸…å–®é …ç›®å‰å¾Œä½¿ç”¨å¤šé¤˜ç¸®æ’æˆ–ç©ºè¡Œï¼Œå› ç‚ºä¹‹å¾Œæœƒå†è¢«æ¸²æŸ“ã€‚
ä»¥ä¸‹æ˜¯å­¸ç”Ÿçš„éœ€æ±‚ï¼š
{question}

ä»¥ä¸‹æ˜¯å¯èƒ½çš„èª²ç¨‹è³‡è¨Šï¼š
{context}

è«‹æ ¹æ“šä¸Šè¿°è³‡è¨Šæ¨è–¦æœ€é©åˆçš„5é–€èª²ï¼Œèªªæ˜åŸå› ï¼Œä¸¦æä¾›èª²åã€æ™‚é–“èˆ‡æµæ°´è™Ÿã€‚
"""
)


def initialize_vectordb():
    global vectordb, qa_chain

    if vectordb is not None:
        return  # å·²åˆå§‹åŒ–éå°±è·³é

    # è‹¥å‘é‡è³‡æ–™å°šæœªä¸‹è¼‰èˆ‡è§£å£“ï¼Œå…ˆè™•ç†
    if not os.path.exists(CHROMA_LOCAL_DIR):
        download_and_extract_chroma_data(container_name=BLOB_CONTAINER_NAME, blob_name=BLOB_FILE_NAME, download_dir=CHROMA_LOCAL_DIR, connection_string=BLOB_CONNECTION_STRING)

    embedding = OpenAIEmbeddings(api_key=OPENAI_API_KEY)
    vectordb = Chroma(persist_directory=CHROMA_LOCAL_DIR, embedding_function=embedding)
    retriever = vectordb.as_retriever(search_kwargs={"k": 5})

    llm = ChatOpenAI(model=OPENAI_MODEL, api_key=SecretStr(OPENAI_API_KEY), temperature=0.3)
    qa_chain = RetrievalQA.from_chain_type(llm=llm, retriever=retriever, return_source_documents=True, chain_type_kwargs={"prompt": prompt})


def clean_markdown(text: str) -> str:
    text = re.sub(r"\n\s*\n+", "\n", text)
    text = re.sub(r"\n[ ]+(?=- )", "\n- ", text)
    text = re.sub(r"(- [^\n]+)\n(?!- )", r"\1 ", text)
    return text.strip()


def recommend_course(question, schedule):
    if vectordb is None or qa_chain is None:
        raise RuntimeError("è«‹å…ˆå‘¼å« initialize_vectordb() åˆå§‹åŒ–å‘é‡è³‡æ–™åº«ã€‚")

    raw_docs = vectordb.similarity_search(question, k=15)
    user_set = set(schedule)

    filtered_docs = []
    for doc in raw_docs:
        time_slots_str = doc.metadata.get("time_slots", "")
        course_slots = time_slots_str.split(",") if time_slots_str else []
        if set(course_slots).issubset(user_set):
            filtered_docs.append(doc)

    if not filtered_docs:
        return "æ‰¾ä¸åˆ°ç¬¦åˆæ‚¨æ™‚æ®µçš„èª²ç¨‹ï¼Œè«‹å˜—è©¦èª¿æ•´æ™‚é–“æˆ–æå•å…§å®¹ã€‚"

    context = "\n\n".join(doc.page_content for doc in filtered_docs[:5])
    filled_prompt = prompt.format(question=question, context=context)

    result = qa_chain.llm.invoke(filled_prompt)
    cleaned = clean_markdown(result.content)

    print("ğŸ§ª result.content (initial):", repr(result.content))
    print("ğŸ§ª result.content (cleaned):", repr(cleaned))
    return cleaned
