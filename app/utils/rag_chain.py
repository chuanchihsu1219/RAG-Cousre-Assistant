# app/utils/rag_chain.py
import os
import re
import markdown
from app.utils.blob_loader import download_and_extract_chroma_data
from langchain_chroma import Chroma
from langchain_openai import ChatOpenAI, OpenAIEmbeddings
from langchain.prompts import PromptTemplate
from langchain.chains import RetrievalQA
from langchain_core.output_parsers import StrOutputParser
from pydantic import SecretStr

# ç’°å¢ƒè¨­å®š
BLOB_CONNECTION_STRING = os.environ["AZURE_BLOB_CONNECTION_STRING"]
BLOB_CONTAINER_NAME = os.environ["AZURE_BLOB_CONTAINER"]
BLOB_FILE_NAME = os.environ["BLOB_NAME"]
OPENAI_API_KEY = os.environ["OPENAI_API_KEY"]
OPENAI_MODEL = os.environ.get("OPENAI_MODEL", "gpt-4o")

CHROMA_LOCAL_DIR = "persist"

# å…¨åŸŸè®Šæ•¸
vectordb = None
qa_chain = None

# Prompt æ¨¡æ¿
prompt = PromptTemplate.from_template(
    """
ä½ æ˜¯å€‹å°ˆæ¥­çš„èª²ç¨‹é¡§å•ï¼Œæœƒæ ¹æ“šå­¸ç”Ÿçš„éœ€æ±‚å¾èª²ç¨‹æè¿°ä¸­æ¨è–¦æœ€é©åˆçš„èª²ã€‚è«‹ä½¿ç”¨ **æ¨™æº– Markdown èªæ³•èˆ‡ç¸®æ’** å›è¦†ï¼Œå‹¿åœ¨æ¸…å–®é …ç›®å‰å¾Œä½¿ç”¨å¤šé¤˜ç¸®æ’æˆ–ç©ºè¡Œï¼Œå› ç‚ºä¹‹å¾Œæœƒå†è¢«æ¸²æŸ“ã€‚
ä»¥ä¸‹æ˜¯å­¸ç”Ÿçš„éœ€æ±‚ï¼š
{question}

ä»¥ä¸‹æ˜¯å¯èƒ½çš„èª²ç¨‹è³‡è¨Šï¼š
{context}

è«‹æ ¹æ“šä¸Šè¿°è³‡è¨Šæ¨è–¦æœ€é©åˆçš„5é–€èª²ï¼Œèªªæ˜åŸå› ï¼Œä¸¦æä¾›èª²åã€æ™‚é–“èˆ‡æµæ°´è™Ÿã€‚
"""
)


def initialize_vectordb():
    print("ğŸ”„ åˆå§‹åŒ–å‘é‡è³‡æ–™åº«...", flush=True)

    global vectordb, qa_chain

    if vectordb is not None:
        return

    if not os.path.exists(CHROMA_LOCAL_DIR):
        download_and_extract_chroma_data(container_name=BLOB_CONTAINER_NAME, blob_name=BLOB_FILE_NAME, download_dir=CHROMA_LOCAL_DIR, connection_string=BLOB_CONNECTION_STRING)

    embedding = OpenAIEmbeddings(api_key=SecretStr(OPENAI_API_KEY))
    vectordb = Chroma(persist_directory=CHROMA_LOCAL_DIR, embedding_function=embedding)
    retriever = vectordb.as_retriever(search_kwargs={"k": 5})

    # Debug å‘é‡ç­†æ•¸
    try:
        print("ğŸ“Š å‘é‡è³‡æ–™ç­†æ•¸ï¼š", len(vectordb.get()["documents"]))
        print("âœ… å‘é‡è³‡æ–™åº«åˆå§‹åŒ–å®Œæˆï¼")
    except Exception as e:
        print("âŒ å‘é‡è®€å–å¤±æ•—ï¼š", e)

    llm = ChatOpenAI(model=OPENAI_MODEL, api_key=SecretStr(OPENAI_API_KEY), temperature=0.3)
    qa_chain = RetrievalQA.from_chain_type(llm=llm, retriever=retriever, return_source_documents=True, chain_type_kwargs={"prompt": prompt})


def clean_markdown(text: str) -> str:
    # text = re.sub(r"\n\s*\n+", "\n", text)
    # text = re.sub(r"\n[ ]+(?=- )", "\n- ", text)
    # text = re.sub(r"(- [^\n]+)\n(?!- )", r"\1 ", text)
    return text.strip()


def recommend_course(question, schedule):
    if vectordb is None or qa_chain is None:
        raise RuntimeError("è«‹å…ˆå‘¼å« initialize_vectordb() åˆå§‹åŒ–å‘é‡è³‡æ–™åº«ã€‚")

    print("âœ… ä½¿ç”¨è€…å•é¡Œï¼š", question)
    print("âœ… ä½¿ç”¨è€…æ™‚æ®µï¼ˆscheduleï¼‰:", schedule)

    # åˆæ­¥æª¢ç´¢
    raw_docs = vectordb.similarity_search(question, k=15)
    print(f"ğŸ” åˆæ­¥ç›¸ä¼¼èª²ç¨‹æ•¸é‡ï¼š{len(raw_docs)}")

    user_set = set(schedule)
    filtered_docs = []

    for i, doc in enumerate(raw_docs):
        metadata = doc.metadata
        print(f"\nğŸ“˜ èª²ç¨‹ {i+1} metadata:", metadata)

        course_slots = metadata.get("time_slots", "").split(",") if metadata.get("time_slots") else []
        print("ğŸ•’ èª²ç¨‹æ™‚æ®µï¼š", course_slots)
        print("ğŸ“‹ æ˜¯å¦ç¬¦åˆä½¿ç”¨è€…æ™‚æ®µï¼Ÿ", set(course_slots).issubset(user_set))

        if set(course_slots).issubset(user_set):
            filtered_docs.append(doc)

    print(f"\nâœ… é€šéæ™‚æ®µéæ¿¾çš„èª²ç¨‹æ•¸é‡ï¼š{len(filtered_docs)}")

    if not filtered_docs:
        return "æ‰¾ä¸åˆ°ç¬¦åˆæ‚¨æ™‚æ®µçš„èª²ç¨‹ï¼Œè«‹å˜—è©¦èª¿æ•´æ™‚é–“æˆ–æå•å…§å®¹ã€‚"

    context = "\n\n".join(doc.page_content for doc in filtered_docs[:5])
    filled_prompt = prompt.format(question=question, context=context)

    result = qa_chain.invoke(filled_prompt)
    raw_output = result.get("result", "")
    cleaned = clean_markdown(raw_output)

    print("ğŸ§ª result.content (initial):", repr(raw_output))
    print("ğŸ§ª result.content (cleaned):", repr(cleaned))

    return cleaned
